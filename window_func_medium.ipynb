{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1600602586578",
   "display_name": "Python 3.6.10 64-bit ('spark-env-2.4.4-v3': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "+---------+----------+-----------+---------------------+-------------------+---------------+\n|  company|department|employee_id|employee_joining_date|employee_leave_date|employee_salary|\n+---------+----------+-----------+---------------------+-------------------+---------------+\n|   oracle|     Sales|          1|           12-05-2020|         12-06-2025|           1000|\n|   oracle|     Sales|          2|           03-07-2020|         20-08-2023|           1200|\n|   oracle|   Finance|          3|           02-08-2020|         12-10-2024|           3000|\n|   oracle|   Finance|          4|           02-09-2020|         17-10-2022|           1800|\n|microsoft|   Finance|          1|           02-05-2020|         02-07-2023|           1400|\n|microsoft|     Sales|          2|           16-05-2020|         16-06-2025|           5000|\n|microsoft|   Finance|          3|           02-07-2020|         02-08-2026|           1600|\n|microsoft|     Sales|          4|           05-09-2020|         23-09-2027|           2600|\n+---------+----------+-----------+---------------------+-------------------+---------------+\n\n"
    }
   ],
   "source": [
    "# from pyspark.sql.functions import to_date,datediff,lit,udf,sum,avg,col,count,lag\n",
    "# from pyspark.sql.types import StringType,LongType,StructType,StructField,DateType,IntegerType,DoubleType\n",
    "# from datetime import datetime\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "df = spark.createDataFrame([\n",
    "    \n",
    "    ('oracle','Sales',1,'12-05-2020','12-06-2025',1000),\n",
    "    ('oracle','Sales',2,'03-07-2020','20-08-2023',1200),\n",
    "    ('oracle','Finance',3,'02-08-2020','12-10-2024', 3000),\n",
    "    ('oracle','Finance',4,'02-09-2020','17-10-2022', 1800),\n",
    "    ('microsoft','Finance', 1,'02-05-2020','02-07-2023', 1400),\n",
    "    ('microsoft','Sales',2,'16-05-2020','16-06-2025', 5000),\n",
    "    ('microsoft','Finance',3,'02-07-2020','02-08-2026', 1600),\n",
    "    ('microsoft','Sales',4,'05-09-2020','23-09-2027', 2600)],\n",
    "    \n",
    "    ['company','department','employee_id','employee_joining_date','employee_leave_date','employee_salary']\n",
    ")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "root\n |-- company: string (nullable = true)\n |-- department: string (nullable = true)\n |-- employee_id: long (nullable = true)\n |-- employee_joining_date: string (nullable = true)\n |-- employee_leave_date: string (nullable = true)\n |-- employee_salary: long (nullable = true)\n\n"
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import to_date\n",
    "\n",
    "df = df.withColumn('employee_joining_date', to_date(df['employee_joining_date'],format='dd-MM-yyyy'))\n",
    "df = df.withColumn('employee_leave_date', to_date(df['employee_leave_date'],format='dd-MM-yyyy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "root\n |-- company: string (nullable = true)\n |-- department: string (nullable = true)\n |-- employee_id: long (nullable = true)\n |-- employee_joining_date: date (nullable = true)\n |-- employee_leave_date: date (nullable = true)\n |-- employee_salary: long (nullable = true)\n\n"
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "source": [
    "## how much company spent on employee salary over time"
   ],
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "window_spec = Window.partitionBy('company').orderBy('employee_joining_date').rowsBetween(Window.unboundedPreceding,Window.currentRow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as func\n",
    "df = df.withColumn('spending_over_emp_salary',func.sum('employee_salary').over(window_spec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "+---------+----------+-----------+---------------------+-------------------+---------------+------------------------+\n|  company|department|employee_id|employee_joining_date|employee_leave_date|employee_salary|spending_over_emp_salary|\n+---------+----------+-----------+---------------------+-------------------+---------------+------------------------+\n|microsoft|   Finance|          1|           2020-05-02|         2023-07-02|           1400|                    1400|\n|microsoft|     Sales|          2|           2020-05-16|         2025-06-16|           5000|                    6400|\n|microsoft|   Finance|          3|           2020-07-02|         2026-08-02|           1600|                    8000|\n|microsoft|     Sales|          4|           2020-09-05|         2027-09-23|           2600|                   10600|\n|   oracle|     Sales|          1|           2020-05-12|         2025-06-12|           1000|                    1000|\n|   oracle|     Sales|          2|           2020-07-03|         2023-08-20|           1200|                    2200|\n|   oracle|   Finance|          3|           2020-08-02|         2024-10-12|           3000|                    5200|\n|   oracle|   Finance|          4|           2020-09-02|         2022-10-17|           1800|                    7000|\n+---------+----------+-----------+---------------------+-------------------+---------------+------------------------+\n\n"
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "source": [
    "## how much companies spend over departments"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "\n",
    "window_spec = Window.partitionBy('department').orderBy('employee_joining_date').rowsBetween(Window.unboundedPreceding,Window.unboundedFollowing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as func\n",
    "\n",
    "df = df.withColumn('dept_level_expenditure', func.sum('employee_salary').over(window_spec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "+---------+----------+---------------------+----------------------+\n|  company|department|employee_joining_date|dept_level_expenditure|\n+---------+----------+---------------------+----------------------+\n|   oracle|     Sales|           2020-05-12|                  9800|\n|microsoft|     Sales|           2020-05-16|                  9800|\n|   oracle|     Sales|           2020-07-03|                  9800|\n|microsoft|     Sales|           2020-09-05|                  9800|\n|microsoft|   Finance|           2020-05-02|                  7800|\n|microsoft|   Finance|           2020-07-02|                  7800|\n|   oracle|   Finance|           2020-08-02|                  7800|\n|   oracle|   Finance|           2020-09-02|                  7800|\n+---------+----------+---------------------+----------------------+\n\n"
    }
   ],
   "source": [
    "df.select('company','department','employee_joining_date','dept_level_expenditure').show()"
   ]
  },
  {
   "source": [
    "## how companies spend over departments increase over time"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "\n",
    "window_spec = Window.partitionBy('department').orderBy('employee_joining_date').rowsBetween(Window.unboundedPreceding,Window.currentRow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as func\n",
    "\n",
    "df = df.withColumn('increase_in_dept_level_expenditure', func.sum('employee_salary').over(window_spec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "+---------+----------+---------------------+----------------------------------+\n|  company|department|employee_joining_date|increase_in_dept_level_expenditure|\n+---------+----------+---------------------+----------------------------------+\n|   oracle|     Sales|           2020-05-12|                              1000|\n|microsoft|     Sales|           2020-05-16|                              6000|\n|   oracle|     Sales|           2020-07-03|                              7200|\n|microsoft|     Sales|           2020-09-05|                              9800|\n|microsoft|   Finance|           2020-05-02|                              1400|\n|microsoft|   Finance|           2020-07-02|                              3000|\n|   oracle|   Finance|           2020-08-02|                              6000|\n|   oracle|   Finance|           2020-09-02|                              7800|\n+---------+----------+---------------------+----------------------------------+\n\n"
    }
   ],
   "source": [
    "df.select('company','department','employee_joining_date','increase_in_dept_level_expenditure').show()"
   ]
  },
  {
   "source": [
    "## how many employee were working with company between employees joining date and left date"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import datediff, lit, sum\n",
    "\n",
    "df = df.withColumn('window_size_helper',lit(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as func\n",
    "\n",
    "left_before_window = Window.partitionBy('company').orderBy('employee_leave_date').rowsBetween(Window.unboundedPreceding, Window.currentRow)\n",
    "\n",
    "n_employee_in_cmpny_window = Window.partitionBy('company').rangeBetween(Window.unboundedPreceding, Window.unboundedFollowing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn('n_employees',func.sum('window_size_helper').over(n_employee_in_cmpny_window) - func.sum('window_size_helper').over(left_before_window) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "+---------+---------------------+-------------------+--------------+-----------+\n|  company|employee_joining_date|employee_leave_date|n_days_working|n_employees|\n+---------+---------------------+-------------------+--------------+-----------+\n|   oracle|           2020-09-02|         2022-10-17|           775|          4|\n|microsoft|           2020-05-02|         2023-07-02|          1156|          4|\n|   oracle|           2020-07-03|         2023-08-20|          1143|          3|\n|   oracle|           2020-08-02|         2024-10-12|          1532|          2|\n|   oracle|           2020-05-12|         2025-06-12|          1857|          1|\n|microsoft|           2020-05-16|         2025-06-16|          1857|          3|\n|microsoft|           2020-07-02|         2026-08-02|          2222|          2|\n|microsoft|           2020-09-05|         2027-09-23|          2574|          1|\n+---------+---------------------+-------------------+--------------+-----------+\n\n"
    }
   ],
   "source": [
    "df.orderBy(['employee_leave_date']).select('company','employee_joining_date','employee_leave_date','n_days_working','n_employees').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "## how many employees have salary in the range of +-1000 of the current employee at a company"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "\n",
    "window_spec = Window.partitionBy('company').orderBy('employee_salary').rangeBetween(-1000,+1000)\n",
    "\n",
    "# -1 to minus current employee\n",
    "df = df.withColumn('n_employees_with_salary+-1000',func.sum('window_size_helper').over(window_spec) - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "+---------+---------------+-----------------------------+\n|  company|employee_salary|n_employees_with_salary+-1000|\n+---------+---------------+-----------------------------+\n|microsoft|           1400|                            1|\n|microsoft|           1600|                            2|\n|microsoft|           2600|                            1|\n|microsoft|           5000|                            0|\n|   oracle|           1000|                            2|\n|   oracle|           1200|                            2|\n|   oracle|           1800|                            2|\n|   oracle|           3000|                            0|\n+---------+---------------+-----------------------------+\n\n"
    }
   ],
   "source": [
    "df.select('company','employee_salary','n_employees_with_salary+-1000').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}